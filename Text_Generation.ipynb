{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_kxtwwVsC0t",
        "outputId": "39663cca-2fc5-4def-a417-08486bcb9636"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.2)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "FrY8bxq5sE6V"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "To be, or not to be, that is the question:\n",
        "Whether 'tis nobler in the mind to suffer\n",
        "The slings and arrows of outrageous fortune,\n",
        "Or to take arms against a sea of troubles\n",
        "And by opposing end them.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Bnm25xfMsHCI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words = len(tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "343mjZ5NsNBa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the word index\n",
        "print(\"\\nStep 1: Word Index (Vocabulary Mapping)\")\n",
        "print(tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlZyNzA8sO8L",
        "outputId": "bc43f453-0871-4c6c-f0ca-810ccb65483e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 1: Word Index (Vocabulary Mapping)\n",
            "{'to': 1, 'the': 2, 'be': 3, 'or': 4, 'and': 5, 'of': 6, 'not': 7, 'that': 8, 'is': 9, 'question': 10, 'whether': 11, \"'tis\": 12, 'nobler': 13, 'in': 14, 'mind': 15, 'suffer': 16, 'slings': 17, 'arrows': 18, 'outrageous': 19, 'fortune': 20, 'take': 21, 'arms': 22, 'against': 23, 'a': 24, 'sea': 25, 'troubles': 26, 'by': 27, 'opposing': 28, 'end': 29, 'them': 30}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences = []\n",
        "for line in text.split('\\n'):\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\n",
        "    # Print tokenized version of each line\n",
        "    print(f\"\\nTokenized line: {line} -> {token_list}\")\n",
        "\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqSBkax6sRuq",
        "outputId": "c15d0e7a-3413-4bc1-cf25-df4124e63efa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokenized line:  -> []\n",
            "\n",
            "Tokenized line: To be, or not to be, that is the question: -> [1, 3, 4, 7, 1, 3, 8, 9, 2, 10]\n",
            "\n",
            "Tokenized line: Whether 'tis nobler in the mind to suffer -> [11, 12, 13, 14, 2, 15, 1, 16]\n",
            "\n",
            "Tokenized line: The slings and arrows of outrageous fortune, -> [2, 17, 5, 18, 6, 19, 20]\n",
            "\n",
            "Tokenized line: Or to take arms against a sea of troubles -> [4, 1, 21, 22, 23, 24, 25, 6, 26]\n",
            "\n",
            "Tokenized line: And by opposing end them. -> [5, 27, 28, 29, 30]\n",
            "\n",
            "Tokenized line:  -> []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Pad sequences and create predictors/labels\n",
        "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "predictors, label = input_sequences[:,:-1], input_sequences[:,-1]\n",
        "label = to_categorical(label, num_classes=total_words)\n",
        "\n",
        "# Print padded sequences\n",
        "print(\"\\nStep 2: Padded Input Sequences\")\n",
        "print(input_sequences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiqrvtgZsoVU",
        "outputId": "a2d69220-9307-4810-8336-16cd9e4cbfef"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 2: Padded Input Sequences\n",
            "[[ 0  0  0  0  0  0  0  0  1  3]\n",
            " [ 0  0  0  0  0  0  0  1  3  4]\n",
            " [ 0  0  0  0  0  0  1  3  4  7]\n",
            " [ 0  0  0  0  0  1  3  4  7  1]\n",
            " [ 0  0  0  0  1  3  4  7  1  3]\n",
            " [ 0  0  0  1  3  4  7  1  3  8]\n",
            " [ 0  0  1  3  4  7  1  3  8  9]\n",
            " [ 0  1  3  4  7  1  3  8  9  2]\n",
            " [ 1  3  4  7  1  3  8  9  2 10]\n",
            " [ 0  0  0  0  0  0  0  0 11 12]\n",
            " [ 0  0  0  0  0  0  0 11 12 13]\n",
            " [ 0  0  0  0  0  0 11 12 13 14]\n",
            " [ 0  0  0  0  0 11 12 13 14  2]\n",
            " [ 0  0  0  0 11 12 13 14  2 15]\n",
            " [ 0  0  0 11 12 13 14  2 15  1]\n",
            " [ 0  0 11 12 13 14  2 15  1 16]\n",
            " [ 0  0  0  0  0  0  0  0  2 17]\n",
            " [ 0  0  0  0  0  0  0  2 17  5]\n",
            " [ 0  0  0  0  0  0  2 17  5 18]\n",
            " [ 0  0  0  0  0  2 17  5 18  6]\n",
            " [ 0  0  0  0  2 17  5 18  6 19]\n",
            " [ 0  0  0  2 17  5 18  6 19 20]\n",
            " [ 0  0  0  0  0  0  0  0  4  1]\n",
            " [ 0  0  0  0  0  0  0  4  1 21]\n",
            " [ 0  0  0  0  0  0  4  1 21 22]\n",
            " [ 0  0  0  0  0  4  1 21 22 23]\n",
            " [ 0  0  0  0  4  1 21 22 23 24]\n",
            " [ 0  0  0  4  1 21 22 23 24 25]\n",
            " [ 0  0  4  1 21 22 23 24 25  6]\n",
            " [ 0  4  1 21 22 23 24 25  6 26]\n",
            " [ 0  0  0  0  0  0  0  0  5 27]\n",
            " [ 0  0  0  0  0  0  0  5 27 28]\n",
            " [ 0  0  0  0  0  0  5 27 28 29]\n",
            " [ 0  0  0  0  0  5 27 28 29 30]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print predictors and labels\n",
        "print(\"\\nStep 2: Predictors (Input) and Labels (Output)\")\n",
        "print(\"Predictors:\\n\", predictors)\n",
        "print(\"Labels:\\n\", label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGSoBrYvuHPd",
        "outputId": "c4d9ed76-8310-46f1-e0f7-2f1de4f085d6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 2: Predictors (Input) and Labels (Output)\n",
            "Predictors:\n",
            " [[ 0  0  0  0  0  0  0  0  1]\n",
            " [ 0  0  0  0  0  0  0  1  3]\n",
            " [ 0  0  0  0  0  0  1  3  4]\n",
            " [ 0  0  0  0  0  1  3  4  7]\n",
            " [ 0  0  0  0  1  3  4  7  1]\n",
            " [ 0  0  0  1  3  4  7  1  3]\n",
            " [ 0  0  1  3  4  7  1  3  8]\n",
            " [ 0  1  3  4  7  1  3  8  9]\n",
            " [ 1  3  4  7  1  3  8  9  2]\n",
            " [ 0  0  0  0  0  0  0  0 11]\n",
            " [ 0  0  0  0  0  0  0 11 12]\n",
            " [ 0  0  0  0  0  0 11 12 13]\n",
            " [ 0  0  0  0  0 11 12 13 14]\n",
            " [ 0  0  0  0 11 12 13 14  2]\n",
            " [ 0  0  0 11 12 13 14  2 15]\n",
            " [ 0  0 11 12 13 14  2 15  1]\n",
            " [ 0  0  0  0  0  0  0  0  2]\n",
            " [ 0  0  0  0  0  0  0  2 17]\n",
            " [ 0  0  0  0  0  0  2 17  5]\n",
            " [ 0  0  0  0  0  2 17  5 18]\n",
            " [ 0  0  0  0  2 17  5 18  6]\n",
            " [ 0  0  0  2 17  5 18  6 19]\n",
            " [ 0  0  0  0  0  0  0  0  4]\n",
            " [ 0  0  0  0  0  0  0  4  1]\n",
            " [ 0  0  0  0  0  0  4  1 21]\n",
            " [ 0  0  0  0  0  4  1 21 22]\n",
            " [ 0  0  0  0  4  1 21 22 23]\n",
            " [ 0  0  0  4  1 21 22 23 24]\n",
            " [ 0  0  4  1 21 22 23 24 25]\n",
            " [ 0  4  1 21 22 23 24 25  6]\n",
            " [ 0  0  0  0  0  0  0  0  5]\n",
            " [ 0  0  0  0  0  0  0  5 27]\n",
            " [ 0  0  0  0  0  0  5 27 28]\n",
            " [ 0  0  0  0  0  5 27 28 29]]\n",
            "Labels:\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 1. 0. 0.]\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " [0. 0. 0. ... 0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Define the LSTM model\n",
        "model = Sequential([\n",
        "    Embedding(total_words, 100, input_length=max_sequence_len - 1),\n",
        "    LSTM(150),\n",
        "    Dense(total_words, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sw545S7LuTFC",
        "outputId": "695046ca-08cc-4184-90bc-731dbc5c13a1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "gN0UonHGufJo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Train the model\n",
        "print(\"\\nStep 4: Training the Model\")\n",
        "model.fit(predictors, label, epochs=100, verbose=1)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NROHjFWLuh0h",
        "outputId": "97621f78-1cb3-4f21-b2ff-a32146cca30f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 4: Training the Model\n",
            "Epoch 1/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 29ms/step - accuracy: 0.0600 - loss: 3.4335\n",
            "Epoch 2/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.0901 - loss: 3.4220\n",
            "Epoch 3/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.0901 - loss: 3.4136\n",
            "Epoch 4/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.0797 - loss: 3.4069\n",
            "Epoch 5/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.0600 - loss: 3.3981\n",
            "Epoch 6/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.0600 - loss: 3.3899\n",
            "Epoch 7/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0600 - loss: 3.3809 \n",
            "Epoch 8/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.0600 - loss: 3.3713\n",
            "Epoch 9/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.0600 - loss: 3.3624 \n",
            "Epoch 10/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.0600 - loss: 3.3531\n",
            "Epoch 11/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.0600 - loss: 3.3447\n",
            "Epoch 12/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.0600 - loss: 3.3359\n",
            "Epoch 13/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.0600 - loss: 3.3223\n",
            "Epoch 14/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.0600 - loss: 3.3075 \n",
            "Epoch 15/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.0600 - loss: 3.2877\n",
            "Epoch 16/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.0600 - loss: 3.2622\n",
            "Epoch 17/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0901 - loss: 3.2531\n",
            "Epoch 18/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.1201 - loss: 3.2316\n",
            "Epoch 19/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.1201 - loss: 3.2157\n",
            "Epoch 20/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.1501 - loss: 3.1970\n",
            "Epoch 21/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.1201 - loss: 3.1783\n",
            "Epoch 22/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.1201 - loss: 3.1460 \n",
            "Epoch 23/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.1801 - loss: 3.1332\n",
            "Epoch 24/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.1801 - loss: 3.0960\n",
            "Epoch 25/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.2598 - loss: 3.0913\n",
            "Epoch 26/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.2898 - loss: 3.0763\n",
            "Epoch 27/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.2702 - loss: 3.0597 \n",
            "Epoch 28/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.2102 - loss: 3.0261\n",
            "Epoch 29/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.1801 - loss: 3.0076 \n",
            "Epoch 30/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.1501 - loss: 2.9598 \n",
            "Epoch 31/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.2402 - loss: 2.9202\n",
            "Epoch 32/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.2598 - loss: 2.8682 \n",
            "Epoch 33/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.2402 - loss: 2.7904\n",
            "Epoch 34/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.2702 - loss: 2.7374 \n",
            "Epoch 35/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.2598 - loss: 2.6806 \n",
            "Epoch 36/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.3002 - loss: 2.6240\n",
            "Epoch 37/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.2898 - loss: 2.5776 \n",
            "Epoch 38/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.2898 - loss: 2.4973\n",
            "Epoch 39/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.2598 - loss: 2.4571\n",
            "Epoch 40/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.2298 - loss: 2.4387\n",
            "Epoch 41/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.2298 - loss: 2.4395 \n",
            "Epoch 42/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.1801 - loss: 2.4229\n",
            "Epoch 43/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.2102 - loss: 2.3434 \n",
            "Epoch 44/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.2898 - loss: 2.2677 \n",
            "Epoch 45/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.2194 - loss: 2.3009 \n",
            "Epoch 46/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.2102 - loss: 2.3378 \n",
            "Epoch 47/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.2298 - loss: 2.3069\n",
            "Epoch 48/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.2298 - loss: 2.1990 \n",
            "Epoch 49/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.2598 - loss: 2.1626 \n",
            "Epoch 50/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.2402 - loss: 2.1290 \n",
            "Epoch 51/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.3002 - loss: 2.0730 \n",
            "Epoch 52/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.3395 - loss: 2.0268 \n",
            "Epoch 53/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.3603 - loss: 1.9417 \n",
            "Epoch 54/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.2298 - loss: 1.9534\n",
            "Epoch 55/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.1998 - loss: 1.9890 \n",
            "Epoch 56/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.2598 - loss: 1.9927 \n",
            "Epoch 57/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.3002 - loss: 1.9125 \n",
            "Epoch 58/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.3303 - loss: 1.8361 \n",
            "Epoch 59/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.3995 - loss: 1.8164 \n",
            "Epoch 60/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4504 - loss: 1.7682 \n",
            "Epoch 61/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.4099 - loss: 1.7519\n",
            "Epoch 62/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4099 - loss: 1.7208 \n",
            "Epoch 63/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5600 - loss: 1.6625\n",
            "Epoch 64/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5300 - loss: 1.6444\n",
            "Epoch 65/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.5901 - loss: 1.5778 \n",
            "Epoch 66/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.5600 - loss: 1.5463 \n",
            "Epoch 67/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5496 - loss: 1.5229\n",
            "Epoch 68/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4700 - loss: 1.5409\n",
            "Epoch 69/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4295 - loss: 1.5696\n",
            "Epoch 70/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.4099 - loss: 1.5407 \n",
            "Epoch 71/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.4400 - loss: 1.5096 \n",
            "Epoch 72/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5000 - loss: 1.4716\n",
            "Epoch 73/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.5196 - loss: 1.4451\n",
            "Epoch 74/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5901 - loss: 1.4234 \n",
            "Epoch 75/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5901 - loss: 1.3742 \n",
            "Epoch 76/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6397 - loss: 1.3450 \n",
            "Epoch 77/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5901 - loss: 1.2782 \n",
            "Epoch 78/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6201 - loss: 1.2491 \n",
            "Epoch 79/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6501 - loss: 1.2291\n",
            "Epoch 80/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6501 - loss: 1.2124 \n",
            "Epoch 81/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6906 - loss: 1.2182 \n",
            "Epoch 82/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6397 - loss: 1.2147\n",
            "Epoch 83/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6097 - loss: 1.1903\n",
            "Epoch 84/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6201 - loss: 1.1796\n",
            "Epoch 85/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6097 - loss: 1.2087 \n",
            "Epoch 86/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5600 - loss: 1.1812 \n",
            "Epoch 87/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5300 - loss: 1.1619 \n",
            "Epoch 88/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5404 - loss: 1.1258\n",
            "Epoch 89/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6501 - loss: 1.1129 \n",
            "Epoch 90/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6998 - loss: 1.0794 \n",
            "Epoch 91/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7598 - loss: 1.0407 \n",
            "Epoch 92/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7102 - loss: 0.9923 \n",
            "Epoch 93/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6697 - loss: 1.0129 \n",
            "Epoch 94/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7102 - loss: 1.0248\n",
            "Epoch 95/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6697 - loss: 1.0425 \n",
            "Epoch 96/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6397 - loss: 1.0161\n",
            "Epoch 97/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6697 - loss: 0.9861 \n",
            "Epoch 98/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6697 - loss: 0.9778 \n",
            "Epoch 99/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7102 - loss: 0.9616 \n",
            "Epoch 100/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7598 - loss: 0.9355 \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e12006a3490>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Function to generate text\n",
        "def generate_text(seed_text, next_words, max_sequence_len):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\n",
        "        # Print the seed text at each generation step\n",
        "        print(f\"Seed Text (Generating): {seed_text}\")\n",
        "\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='pre')\n",
        "        predicted = np.argmax(model.predict(token_list, verbose=0), axis=-1)\n",
        "        output_word = tokenizer.index_word.get(predicted[0], '')\n",
        "        seed_text += \" \" + output_word\n",
        "    return seed_text\n"
      ],
      "metadata": {
        "id": "THB95u0PuqIC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Generate some text\n",
        "seed_text = \"To be or not to be that is the\"\n",
        "print(\"\\nStep 6: Generated Text\")\n",
        "generated_text = generate_text(seed_text, 1, max_sequence_len)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Do5kQJ02u23L",
        "outputId": "16d49b67-f84e-430a-f6b8-ba553e6c614c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 6: Generated Text\n",
            "Seed Text (Generating): To be or not to be that is the\n",
            "To be or not to be that is the question\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "5K6NigWuu4zD"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load and preprocess the dataset\n",
        "# Load dataset from a file\n",
        "with open('Sonnet.txt', 'r') as file:\n",
        "    text = file.read()"
      ],
      "metadata": {
        "id": "044VR4LbwPy6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nStep 1: Sample of the Dataset\")\n",
        "print(text[:500])  # Print the first 500 characters of the dataset\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNME3whCwYM2",
        "outputId": "9adac342-4982-4be3-c0a8-c8ad1ecf484d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 1: Sample of the Dataset\n",
            "﻿THE SONNETS\n",
            "\n",
            "by William Shakespeare\n",
            "\n",
            "\n",
            "From fairest creatures we desire increase,\n",
            "That thereby beauty’s rose might never die,\n",
            "But as the riper should by time decease,\n",
            "His tender heir might bear his memory:\n",
            "But thou, contracted to thine own bright eyes,\n",
            "Feed’st thy light’s flame with self-substantial fuel,\n",
            "Making a famine where abundance lies,\n",
            "Thyself thy foe, to thy sweet self too cruel:\n",
            "Thou that art now the world’s fresh ornament,\n",
            "And only herald to the gaudy spring,\n",
            "Within thine own bud burie\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words = len(tokenizer.word_index) + 1\n"
      ],
      "metadata": {
        "id": "Xl3ZfQCjwtef"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print word index size\n",
        "print(f\"\\nStep 2: Total Words in Vocabulary: {total_words}\")\n",
        "print(\"Word Index (Sample):\", list(tokenizer.word_index.items())[:10])  # Print a sample of the word index\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWNMi9HNyxtZ",
        "outputId": "9d995950-0580-4abd-b240-fb133427a316"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 2: Total Words in Vocabulary: 3235\n",
            "Word Index (Sample): [('and', 1), ('the', 2), ('to', 3), ('my', 4), ('of', 5), ('i', 6), ('that', 7), ('in', 8), ('thy', 9), ('thou', 10)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text to sequences of tokens\n",
        "input_sequences = []\n",
        "for line in text.split('\\n'):\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\n",
        "    # Skip empty lines\n",
        "    if not token_list:\n",
        "        continue\n",
        "\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n"
      ],
      "metadata": {
        "id": "p0wO-TNayz29"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlvEAxF-5HxX",
        "outputId": "91eea7c1-9b04-484e-e853-b5bb48520b23"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1375, 1376],\n",
              " [30, 1377],\n",
              " [30, 1377, 1378],\n",
              " [34, 414],\n",
              " [34, 414, 869],\n",
              " [34, 414, 869, 165],\n",
              " [34, 414, 869, 165, 214],\n",
              " [34, 414, 869, 165, 214, 512],\n",
              " [7, 870],\n",
              " [7, 870, 133],\n",
              " [7, 870, 133, 350],\n",
              " [7, 870, 133, 350, 101],\n",
              " [7, 870, 133, 350, 101, 154],\n",
              " [7, 870, 133, 350, 101, 154, 199],\n",
              " [18, 22],\n",
              " [18, 22, 2],\n",
              " [18, 22, 2, 871],\n",
              " [18, 22, 2, 871, 61],\n",
              " [18, 22, 2, 871, 61, 30],\n",
              " [18, 22, 2, 871, 61, 30, 48],\n",
              " [18, 22, 2, 871, 61, 30, 48, 635],\n",
              " [26, 312],\n",
              " [26, 312, 636],\n",
              " [26, 312, 636, 101],\n",
              " [26, 312, 636, 101, 200],\n",
              " [26, 312, 636, 101, 200, 26],\n",
              " [26, 312, 636, 101, 200, 26, 278],\n",
              " [18, 10],\n",
              " [18, 10, 872],\n",
              " [18, 10, 872, 3],\n",
              " [18, 10, 872, 3, 62],\n",
              " [18, 10, 872, 3, 62, 84],\n",
              " [18, 10, 872, 3, 62, 84, 215],\n",
              " [18, 10, 872, 3, 62, 84, 215, 51],\n",
              " [1379, 9],\n",
              " [1379, 9, 1380],\n",
              " [1379, 9, 1380, 637],\n",
              " [1379, 9, 1380, 637, 11],\n",
              " [1379, 9, 1380, 637, 11, 85],\n",
              " [1379, 9, 1380, 637, 11, 85, 1381],\n",
              " [1379, 9, 1380, 637, 11, 85, 1381, 1382],\n",
              " [201, 16],\n",
              " [201, 16, 1383],\n",
              " [201, 16, 1383, 68],\n",
              " [201, 16, 1383, 68, 513],\n",
              " [201, 16, 1383, 68, 513, 202],\n",
              " [118, 9],\n",
              " [118, 9, 1384],\n",
              " [118, 9, 1384, 3],\n",
              " [118, 9, 1384, 3, 9],\n",
              " [118, 9, 1384, 3, 9, 47],\n",
              " [118, 9, 1384, 3, 9, 47, 85],\n",
              " [118, 9, 1384, 3, 9, 47, 85, 134],\n",
              " [118, 9, 1384, 3, 9, 47, 85, 134, 279],\n",
              " [10, 7],\n",
              " [10, 7, 54],\n",
              " [10, 7, 54, 60],\n",
              " [10, 7, 54, 60, 2],\n",
              " [10, 7, 54, 60, 2, 415],\n",
              " [10, 7, 54, 60, 2, 415, 313],\n",
              " [10, 7, 54, 60, 2, 415, 313, 416],\n",
              " [1, 351],\n",
              " [1, 351, 1385],\n",
              " [1, 351, 1385, 3],\n",
              " [1, 351, 1385, 3, 2],\n",
              " [1, 351, 1385, 3, 2, 1386],\n",
              " [1, 351, 1385, 3, 2, 1386, 417],\n",
              " [216, 62],\n",
              " [216, 62, 84],\n",
              " [216, 62, 84, 873],\n",
              " [216, 62, 84, 873, 1387],\n",
              " [216, 62, 84, 873, 1387, 9],\n",
              " [216, 62, 84, 873, 1387, 9, 874],\n",
              " [1, 312],\n",
              " [1, 312, 875],\n",
              " [1, 312, 875, 876],\n",
              " [1, 312, 875, 876, 314],\n",
              " [1, 312, 875, 876, 314, 8],\n",
              " [1, 312, 875, 876, 314, 8, 1388],\n",
              " [256, 2],\n",
              " [256, 2, 94],\n",
              " [256, 2, 94, 36],\n",
              " [256, 2, 94, 36, 418],\n",
              " [256, 2, 94, 36, 418, 29],\n",
              " [256, 2, 94, 36, 418, 29, 1389],\n",
              " [256, 2, 94, 36, 418, 29, 1389, 21],\n",
              " [3, 638],\n",
              " [3, 638, 2],\n",
              " [3, 638, 2, 415],\n",
              " [3, 638, 2, 415, 352],\n",
              " [3, 638, 2, 415, 352, 30],\n",
              " [3, 638, 2, 415, 352, 30, 2],\n",
              " [3, 638, 2, 415, 352, 30, 2, 639],\n",
              " [3, 638, 2, 415, 352, 30, 2, 639, 1],\n",
              " [3, 638, 2, 415, 352, 30, 2, 639, 1, 19],\n",
              " [27, 1390],\n",
              " [27, 1390, 877],\n",
              " [27, 1390, 877, 46],\n",
              " [27, 1390, 877, 46, 878],\n",
              " [27, 1390, 877, 46, 878, 9],\n",
              " [27, 1390, 877, 46, 878, 9, 280],\n",
              " [1, 1391],\n",
              " [1, 1391, 281],\n",
              " [1, 1391, 281, 1392],\n",
              " [1, 1391, 281, 1392, 8],\n",
              " [1, 1391, 281, 1392, 8, 9],\n",
              " [1, 1391, 281, 1392, 8, 9, 133],\n",
              " [1, 1391, 281, 1392, 8, 9, 133, 1393],\n",
              " [9, 1394],\n",
              " [9, 1394, 179],\n",
              " [9, 1394, 179, 1395],\n",
              " [9, 1394, 179, 1395, 20],\n",
              " [9, 1394, 179, 1395, 20, 1396],\n",
              " [9, 1394, 179, 1395, 20, 1396, 35],\n",
              " [9, 1394, 179, 1395, 20, 1396, 35, 60],\n",
              " [49, 21],\n",
              " [49, 21, 16],\n",
              " [49, 21, 16, 879],\n",
              " [49, 21, 16, 879, 640],\n",
              " [49, 21, 16, 879, 640, 5],\n",
              " [49, 21, 16, 879, 640, 5, 880],\n",
              " [49, 21, 16, 879, 640, 5, 880, 135],\n",
              " [49, 21, 16, 879, 640, 5, 880, 135, 881],\n",
              " [39, 81],\n",
              " [39, 81, 1397],\n",
              " [39, 81, 1397, 68],\n",
              " [39, 81, 1397, 68, 23],\n",
              " [39, 81, 1397, 68, 23, 9],\n",
              " [39, 81, 1397, 68, 23, 9, 52],\n",
              " [39, 81, 1397, 68, 23, 9, 52, 202],\n",
              " [68, 23],\n",
              " [68, 23, 2],\n",
              " [68, 23, 2, 257],\n",
              " [68, 23, 2, 257, 5],\n",
              " [68, 23, 2, 257, 5, 9],\n",
              " [68, 23, 2, 257, 5, 9, 882],\n",
              " [68, 23, 2, 257, 5, 9, 882, 143],\n",
              " [3, 95],\n",
              " [3, 95, 216],\n",
              " [3, 95, 216, 62],\n",
              " [3, 95, 216, 62, 84],\n",
              " [3, 95, 216, 62, 84, 281],\n",
              " [3, 95, 216, 62, 84, 281, 1398],\n",
              " [3, 95, 216, 62, 84, 281, 1398, 51],\n",
              " [86, 144],\n",
              " [86, 144, 23],\n",
              " [86, 144, 23, 1399],\n",
              " [86, 144, 23, 1399, 232],\n",
              " [86, 144, 23, 1399, 232, 1],\n",
              " [86, 144, 23, 1399, 232, 1, 1400],\n",
              " [86, 144, 23, 1399, 232, 1, 1400, 96],\n",
              " [71, 145],\n",
              " [71, 145, 43],\n",
              " [71, 145, 43, 96],\n",
              " [71, 145, 43, 96, 1401],\n",
              " [71, 145, 43, 96, 1401, 9],\n",
              " [71, 145, 43, 96, 1401, 9, 133],\n",
              " [71, 145, 43, 96, 1401, 9, 133, 186],\n",
              " [42, 10],\n",
              " [42, 10, 1402],\n",
              " [42, 10, 1402, 641],\n",
              " [42, 10, 1402, 641, 883],\n",
              " [42, 10, 1402, 641, 883, 69],\n",
              " [42, 10, 1402, 641, 883, 69, 282],\n",
              " [42, 10, 1402, 641, 883, 69, 282, 5],\n",
              " [42, 10, 1402, 641, 883, 69, 282, 5, 44],\n",
              " [46, 514],\n",
              " [46, 514, 4],\n",
              " [46, 514, 4, 515],\n",
              " [46, 514, 4, 515, 1],\n",
              " [46, 514, 4, 515, 1, 64],\n",
              " [46, 514, 4, 515, 1, 64, 4],\n",
              " [46, 514, 4, 515, 1, 64, 4, 113],\n",
              " [46, 514, 4, 515, 1, 64, 4, 113, 315],\n",
              " [46, 514, 4, 515, 1, 64, 4, 113, 315, 155],\n",
              " [1403, 26],\n",
              " [1403, 26, 52],\n",
              " [1403, 26, 52, 30],\n",
              " [1403, 26, 52, 30, 1404],\n",
              " [1403, 26, 52, 30, 1404, 62],\n",
              " [29, 86],\n",
              " [29, 86, 3],\n",
              " [29, 86, 3, 21],\n",
              " [29, 86, 3, 21, 102],\n",
              " [29, 86, 3, 21, 102, 126],\n",
              " [29, 86, 3, 21, 102, 126, 27],\n",
              " [29, 86, 3, 21, 102, 126, 27, 10],\n",
              " [29, 86, 3, 21, 102, 126, 27, 10, 54],\n",
              " [29, 86, 3, 21, 102, 126, 27, 10, 54, 113],\n",
              " [1, 76],\n",
              " [1, 76, 9],\n",
              " [1, 76, 9, 283],\n",
              " [1, 76, 9, 283, 1405],\n",
              " [1, 76, 9, 283, 1405, 27],\n",
              " [1, 76, 9, 283, 1405, 27, 10],\n",
              " [1, 76, 9, 283, 1405, 27, 10, 1406],\n",
              " [1, 76, 9, 283, 1405, 27, 10, 1406, 28],\n",
              " [1, 76, 9, 283, 1405, 27, 10, 1406, 28, 316],\n",
              " [114, 8],\n",
              " [114, 8, 9],\n",
              " [114, 8, 9, 233],\n",
              " [114, 8, 9, 233, 1],\n",
              " [114, 8, 9, 233, 1, 166],\n",
              " [114, 8, 9, 233, 1, 166, 2],\n",
              " [114, 8, 9, 233, 1, 166, 2, 127],\n",
              " [114, 8, 9, 233, 1, 166, 2, 127, 10],\n",
              " [114, 8, 9, 233, 1, 166, 2, 127, 10, 1407],\n",
              " [60, 13],\n",
              " [60, 13, 2],\n",
              " [60, 13, 2, 48],\n",
              " [60, 13, 2, 48, 7],\n",
              " [60, 13, 2, 48, 7, 127],\n",
              " [60, 13, 2, 48, 7, 127, 61],\n",
              " [60, 13, 2, 48, 7, 127, 61, 217],\n",
              " [60, 13, 2, 48, 7, 127, 61, 217, 258],\n",
              " [128, 313],\n",
              " [128, 313, 642],\n",
              " [128, 313, 642, 42],\n",
              " [128, 313, 642, 42, 60],\n",
              " [128, 313, 642, 42, 60, 10],\n",
              " [128, 313, 642, 42, 60, 10, 14],\n",
              " [128, 313, 642, 42, 60, 10, 14, 1408],\n",
              " [10, 87],\n",
              " [10, 87, 1409],\n",
              " [10, 87, 1409, 2],\n",
              " [10, 87, 1409, 2, 94],\n",
              " [10, 87, 1409, 2, 94, 1410],\n",
              " [10, 87, 1409, 2, 94, 1410, 82],\n",
              " [10, 87, 1409, 2, 94, 1410, 82, 884],\n",
              " [12, 68],\n",
              " [12, 68, 13],\n",
              " [12, 68, 13, 78],\n",
              " [12, 68, 13, 78, 20],\n",
              " [12, 68, 13, 78, 20, 69],\n",
              " [12, 68, 13, 78, 20, 69, 128],\n",
              " [12, 68, 13, 78, 20, 69, 128, 1411],\n",
              " [12, 68, 13, 78, 20, 69, 128, 1411, 885],\n",
              " [1412, 2],\n",
              " [1412, 2, 1413],\n",
              " [1412, 2, 1413, 5],\n",
              " [1412, 2, 1413, 5, 9],\n",
              " [1412, 2, 1413, 5, 9, 886],\n",
              " [36, 88],\n",
              " [36, 88, 13],\n",
              " [36, 88, 13, 65],\n",
              " [36, 88, 13, 65, 20],\n",
              " [36, 88, 13, 65, 20, 887],\n",
              " [36, 88, 13, 65, 20, 887, 49],\n",
              " [36, 88, 13, 65, 20, 887, 49, 21],\n",
              " [36, 88, 13, 65, 20, 887, 49, 21, 2],\n",
              " [36, 88, 13, 65, 20, 887, 49, 21, 2, 419],\n",
              " [5, 26],\n",
              " [5, 26, 85],\n",
              " [5, 26, 85, 15],\n",
              " [5, 26, 85, 15, 3],\n",
              " [5, 26, 85, 15, 3, 888],\n",
              " [5, 26, 85, 15, 3, 888, 643],\n",
              " [10, 54],\n",
              " [10, 54, 9],\n",
              " [10, 54, 9, 644],\n",
              " [10, 54, 9, 644, 233],\n",
              " [10, 54, 9, 644, 233, 1],\n",
              " [10, 54, 9, 644, 233, 1, 78],\n",
              " [10, 54, 9, 644, 233, 1, 78, 8],\n",
              " [10, 54, 9, 644, 233, 1, 78, 8, 19],\n",
              " [420, 259],\n",
              " [420, 259, 2],\n",
              " [420, 259, 2, 284],\n",
              " [420, 259, 2, 284, 645],\n",
              " [420, 259, 2, 284, 645, 5],\n",
              " [420, 259, 2, 284, 645, 5, 55],\n",
              " [420, 259, 2, 284, 645, 5, 55, 516],\n",
              " [20, 10],\n",
              " [20, 10, 317],\n",
              " [20, 10, 317, 646],\n",
              " [20, 10, 317, 646, 5],\n",
              " [20, 10, 317, 646, 5, 62],\n",
              " [20, 10, 317, 646, 5, 62, 167],\n",
              " [20, 10, 317, 646, 5, 62, 167, 218],\n",
              " [20, 10, 317, 646, 5, 62, 167, 218, 76],\n",
              " [353, 5],\n",
              " [353, 5, 421],\n",
              " [353, 5, 421, 29],\n",
              " [353, 5, 421, 29, 9],\n",
              " [353, 5, 421, 29, 9, 422],\n",
              " [353, 5, 421, 29, 9, 422, 48],\n",
              " [18, 42],\n",
              " [18, 42, 10],\n",
              " [18, 42, 10, 89],\n",
              " [18, 42, 10, 89, 647],\n",
              " [18, 42, 10, 89, 647, 14],\n",
              " [18, 42, 10, 89, 647, 14, 3],\n",
              " [18, 42, 10, 89, 647, 14, 3, 21],\n",
              " [199, 517],\n",
              " [199, 517, 1],\n",
              " [199, 517, 1, 62],\n",
              " [199, 517, 1, 62, 518],\n",
              " [199, 517, 1, 62, 518, 1414],\n",
              " [199, 517, 1, 62, 518, 1414, 11],\n",
              " [199, 517, 1, 62, 518, 1414, 11, 19],\n",
              " [1415, 1416],\n",
              " [1415, 1416, 107],\n",
              " [1415, 1416, 107, 87],\n",
              " [1415, 1416, 107, 87, 10],\n",
              " [1415, 1416, 107, 87, 10, 423],\n",
              " [90, 118],\n",
              " [90, 118, 9],\n",
              " [90, 118, 9, 133],\n",
              " [90, 118, 9, 133, 1417],\n",
              " [354, 1418],\n",
              " [354, 1418, 285],\n",
              " [354, 1418, 285, 129],\n",
              " [354, 1418, 285, 129, 18],\n",
              " [354, 1418, 285, 129, 18, 32],\n",
              " [354, 1418, 285, 129, 18, 32, 424],\n",
              " [1, 81],\n",
              " [1, 81, 1419],\n",
              " [1, 81, 1419, 78],\n",
              " [1, 81, 1419, 78, 519],\n",
              " [1, 81, 1419, 78, 519, 3],\n",
              " [1, 81, 1419, 78, 519, 3, 79],\n",
              " [1, 81, 1419, 78, 519, 3, 79, 41],\n",
              " [1, 81, 1419, 78, 519, 3, 79, 41, 520],\n",
              " [39, 260],\n",
              " [39, 260, 889],\n",
              " [39, 260, 889, 107],\n",
              " [39, 260, 889, 107, 87],\n",
              " [39, 260, 889, 107, 87, 10],\n",
              " [39, 260, 889, 107, 87, 10, 648],\n",
              " [2, 890],\n",
              " [2, 890, 1420],\n",
              " [2, 890, 1420, 521],\n",
              " [2, 890, 1420, 521, 19],\n",
              " [2, 890, 1420, 521, 19, 3],\n",
              " [2, 890, 1420, 521, 19, 3, 98],\n",
              " [1421, 891],\n",
              " [1421, 891, 107],\n",
              " [1421, 891, 107, 87],\n",
              " [1421, 891, 107, 87, 10],\n",
              " [1421, 891, 107, 87, 10, 186],\n",
              " [20, 234],\n",
              " [20, 234, 16],\n",
              " [20, 234, 16, 514],\n",
              " [20, 234, 16, 514, 5],\n",
              " [20, 234, 16, 514, 5, 1422],\n",
              " [20, 234, 16, 514, 5, 1422, 56],\n",
              " [20, 234, 16, 514, 5, 1422, 56, 318],\n",
              " [20, 234, 16, 514, 5, 1422, 56, 318, 14],\n",
              " [20, 234, 16, 514, 5, 1422, 56, 318, 14, 89],\n",
              " [12, 355],\n",
              " [12, 355, 1423],\n",
              " [12, 355, 1423, 11],\n",
              " [12, 355, 1423, 11, 118],\n",
              " [12, 355, 1423, 11, 118, 130],\n",
              " [10, 5],\n",
              " [10, 5, 118],\n",
              " [10, 5, 118, 9],\n",
              " [10, 5, 118, 9, 47],\n",
              " [10, 5, 118, 9, 47, 85],\n",
              " [10, 5, 118, 9, 47, 85, 87],\n",
              " [10, 5, 118, 9, 47, 85, 87, 892],\n",
              " [39, 71],\n",
              " [39, 71, 27],\n",
              " [39, 71, 27, 235],\n",
              " [39, 71, 27, 235, 420],\n",
              " [39, 71, 27, 235, 420, 19],\n",
              " [39, 71, 27, 235, 420, 19, 3],\n",
              " [39, 71, 27, 235, 420, 19, 3, 21],\n",
              " [39, 71, 27, 235, 420, 19, 3, 21, 261],\n",
              " [40, 1424],\n",
              " [40, 1424, 649],\n",
              " [40, 1424, 649, 318],\n",
              " [40, 1424, 649, 318, 10],\n",
              " [40, 1424, 649, 318, 10, 236],\n",
              " [9, 522],\n",
              " [9, 522, 52],\n",
              " [9, 522, 52, 119],\n",
              " [9, 522, 52, 119, 21],\n",
              " [9, 522, 52, 119, 21, 1425],\n",
              " [9, 522, 52, 119, 21, 1425, 11],\n",
              " [9, 522, 52, 119, 21, 1425, 11, 19],\n",
              " [25, 1426],\n",
              " [25, 1426, 262],\n",
              " [25, 1426, 262, 893],\n",
              " [25, 1426, 262, 893, 1427],\n",
              " [25, 1426, 262, 893, 1427, 3],\n",
              " [25, 1426, 262, 893, 1427, 3, 21],\n",
              " [79, 203],\n",
              " [79, 203, 7],\n",
              " [79, 203, 7, 11],\n",
              " [79, 203, 7, 11, 180],\n",
              " [79, 203, 7, 11, 180, 650],\n",
              " [79, 203, 7, 11, 180, 650, 103],\n",
              " [79, 203, 7, 11, 180, 650, 103, 523],\n",
              " [2, 284],\n",
              " [2, 284, 894],\n",
              " [2, 284, 894, 68],\n",
              " [2, 284, 894, 68, 91],\n",
              " [2, 284, 894, 68, 91, 74],\n",
              " [2, 284, 894, 68, 91, 74, 32],\n",
              " [2, 284, 894, 68, 91, 74, 32, 356],\n",
              " [49, 425],\n",
              " [49, 425, 2],\n",
              " [49, 425, 2, 1428],\n",
              " [49, 425, 2, 1428, 3],\n",
              " [49, 425, 2, 1428, 3, 2],\n",
              " [49, 425, 2, 1428, 3, 2, 319],\n",
              " [49, 425, 2, 1428, 3, 2, 319, 320],\n",
              " [1, 7],\n",
              " [1, 7, 1429],\n",
              " [1, 7, 1429, 25],\n",
              " [1, 7, 1429, 25, 1430],\n",
              " [1, 7, 1429, 25, 1430, 32],\n",
              " [1, 7, 1429, 25, 1430, 32, 1431],\n",
              " [12, 154],\n",
              " [12, 154, 1432],\n",
              " [12, 154, 1432, 48],\n",
              " [12, 154, 1432, 48, 895],\n",
              " [12, 154, 1432, 48, 895, 286],\n",
              " [12, 154, 1432, 48, 895, 286, 35],\n",
              " [3, 896],\n",
              " [3, 896, 426],\n",
              " [3, 896, 426, 1],\n",
              " [3, 896, 426, 1, 651],\n",
              " [3, 896, 426, 1, 651, 72],\n",
              " [3, 896, 426, 1, 651, 72, 136],\n",
              " [897, 898],\n",
              " [897, 898, 11],\n",
              " [897, 898, 11, 1433],\n",
              " [897, 898, 11, 1433, 1],\n",
              " [897, 898, 11, 1433, 1, 882],\n",
              " [897, 898, 11, 1433, 1, 882, 287],\n",
              " [897, 898, 11, 1433, 1, 882, 287, 427],\n",
              " [897, 898, 11, 1433, 1, 882, 287, 427, 261],\n",
              " [52, 263],\n",
              " [52, 263, 1434],\n",
              " [52, 263, 1434, 1],\n",
              " [52, 263, 1434, 1, 899],\n",
              " [52, 263, 1434, 1, 899, 91],\n",
              " [52, 263, 1434, 1, 899, 91, 68],\n",
              " [39, 86],\n",
              " [39, 86, 14],\n",
              " [39, 86, 14, 219],\n",
              " [39, 86, 14, 219, 1435],\n",
              " [39, 86, 14, 219, 1435, 428],\n",
              " [16, 1436],\n",
              " [16, 1436, 1437],\n",
              " [16, 1436, 1437, 900],\n",
              " [16, 1436, 1437, 900, 8],\n",
              " [16, 1436, 1437, 900, 8, 901],\n",
              " [16, 1436, 1437, 900, 8, 901, 5],\n",
              " [16, 1436, 1437, 900, 8, 901, 5, 233],\n",
              " [133, 652],\n",
              " [133, 652, 11],\n",
              " [133, 652, 11, 52],\n",
              " [133, 652, 11, 52, 86],\n",
              " [133, 652, 11, 52, 86, 1438],\n",
              " [53, 28],\n",
              " [53, 28, 53],\n",
              " [53, 28, 53, 37],\n",
              " [53, 28, 53, 37, 902],\n",
              " [53, 28, 53, 37, 902, 40],\n",
              " [53, 28, 53, 37, 902, 40, 28],\n",
              " [53, 28, 53, 37, 902, 40, 28, 92],\n",
              " [18, 288],\n",
              " [18, 288, 653],\n",
              " [18, 288, 653, 80],\n",
              " [18, 288, 653, 80, 50],\n",
              " [18, 288, 653, 80, 50, 11],\n",
              " [18, 288, 653, 80, 50, 11, 426],\n",
              " [18, 288, 653, 80, 50, 11, 426, 903],\n",
              " [1439, 18],\n",
              " [1439, 18, 45],\n",
              " [1439, 18, 45, 109],\n",
              " [1439, 18, 45, 109, 45],\n",
              " [1439, 18, 45, 109, 45, 524],\n",
              " [1439, 18, 45, 109, 45, 524, 70],\n",
              " [1439, 18, 45, 109, 45, 524, 70, 262],\n",
              " [1439, 18, 45, 109, 45, 524, 70, 262, 47],\n",
              " [39, 104],\n",
              " [39, 104, 14],\n",
              " [39, 104, 14, 654],\n",
              " [39, 104, 14, 654, 1440],\n",
              " [39, 104, 14, 654, 1440, 137],\n",
              " [39, 104, 14, 654, 1440, 137, 1441],\n",
              " [8, 19],\n",
              " [8, 19, 9],\n",
              " [8, 19, 9, 286],\n",
              " [8, 19, 9, 286, 357],\n",
              " [8, 19, 9, 286, 357, 10],\n",
              " [8, 19, 9, 286, 357, 10, 21],\n",
              " [8, 19, 9, 286, 357, 10, 21, 653],\n",
              " [64, 47],\n",
              " [64, 47, 82],\n",
              " [64, 47, 82, 1442],\n",
              " [64, 47, 82, 1442, 257],\n",
              " [64, 47, 82, 1442, 257, 10],\n",
              " [64, 47, 82, 1442, 257, 10, 82],\n",
              " [64, 47, 82, 1442, 257, 10, 82, 237],\n",
              " [11, 133],\n",
              " [11, 133, 257],\n",
              " [11, 133, 257, 357],\n",
              " [11, 133, 257, 357, 28],\n",
              " [11, 133, 257, 357, 28, 21],\n",
              " [11, 133, 257, 357, 28, 21, 85],\n",
              " [11, 133, 257, 357, 28, 21, 85, 1443],\n",
              " [7, 186],\n",
              " [7, 186, 13],\n",
              " [7, 186, 13, 14],\n",
              " [7, 186, 13, 14, 1444],\n",
              " [7, 186, 13, 14, 1444, 1445],\n",
              " [25, 1446],\n",
              " [25, 1446, 79],\n",
              " [25, 1446, 79, 7],\n",
              " [25, 1446, 79, 7, 655],\n",
              " [25, 1446, 79, 7, 655, 2],\n",
              " [25, 1446, 79, 7, 655, 2, 904],\n",
              " [25, 1446, 79, 7, 655, 2, 904, 1447],\n",
              " [1448, 12],\n",
              " [1448, 12, 118],\n",
              " [1448, 12, 118, 3],\n",
              " [1448, 12, 118, 3, 905],\n",
              " [1448, 12, 118, 3, 905, 258],\n",
              " [1448, 12, 118, 3, 905, 258, 19],\n",
              " [36, 321],\n",
              " [36, 321, 264],\n",
              " [36, 321, 264, 656],\n",
              " [36, 321, 264, 656, 21],\n",
              " [36, 321, 264, 656, 21, 28],\n",
              " [36, 321, 264, 656, 21, 28, 321],\n",
              " [36, 321, 264, 656, 21, 28, 321, 12],\n",
              " [36, 321, 264, 656, 21, 28, 321, 12, 66],\n",
              " [321, 264],\n",
              " [321, 264, 118],\n",
              " [321, 264, 118, 86],\n",
              " [321, 264, 118, 86, 656],\n",
              " [321, 264, 118, 86, 656, 59],\n",
              " [321, 264, 118, 86, 656, 59, 10],\n",
              " [321, 264, 118, 86, 656, 59, 10, 54],\n",
              " [42, 321],\n",
              " [42, 321, 5],\n",
              " [42, 321, 5, 62],\n",
              " [42, 321, 5, 62, 321],\n",
              " [42, 321, 5, 62, 321, 264],\n",
              " [42, 321, 5, 62, 321, 264, 1449],\n",
              " [42, 321, 5, 62, 321, 264, 1449, 19],\n",
              " [39, 40],\n",
              " [39, 40, 238],\n",
              " [39, 40, 238, 156],\n",
              " [39, 40, 238, 156, 33],\n",
              " [39, 40, 238, 156, 33, 42],\n",
              " [39, 40, 238, 156, 33, 42, 10],\n",
              " [39, 40, 238, 156, 33, 42, 10, 358],\n",
              " [39, 40, 238, 156, 33, 42, 10, 358, 906],\n",
              " [1450, 19],\n",
              " [1450, 19, 322],\n",
              " [1450, 19, 322, 8],\n",
              " [1450, 19, 322, 8, 643],\n",
              " [21, 14],\n",
              " [21, 14, 85],\n",
              " [21, 14, 85, 1451],\n",
              " [21, 14, 85, 1451, 12],\n",
              " [21, 14, 85, 1451, 12, 10],\n",
              " [21, 14, 85, 1451, 12, 10, 54],\n",
              " [21, 14, 85, 1451, 12, 10, 54, 145],\n",
              " [21, 14, 85, 1451, 12, 10, 54, 145, 134],\n",
              " [21, 14, 85, 1451, 12, 10, 54, 145, 134, 69],\n",
              " [3, 21],\n",
              " [3, 21, 525],\n",
              " [3, 21, 525, 657],\n",
              " [3, 21, 525, 657, 1],\n",
              " [3, 21, 525, 657, 1, 64],\n",
              " [3, 21, 525, 657, 1, 64, 526],\n",
              " [3, 21, 525, 657, 1, 64, 526, 62],\n",
              " [3, 21, 525, 657, 1, 64, 526, 62, 636],\n",
              " [658, 8],\n",
              " [658, 8, 2],\n",
              " [658, 8, 2, 1452],\n",
              " [658, 8, 2, 1452, 27],\n",
              " [658, 8, 2, 1452, 27, 2],\n",
              " [658, 8, 2, 1452, 27, 2, 429],\n",
              " [658, 8, 2, 1452, 27, 2, 429, 359],\n",
              " [1453, 146],\n",
              " [1453, 146, 26],\n",
              " [1453, 146, 26, 1454],\n",
              " [1453, 146, 26, 1454, 360],\n",
              " [1453, 146, 26, 1454, 360, 168],\n",
              " [1453, 146, 26, 1454, 360, 168, 361],\n",
              " [1453, 146, 26, 1454, 360, 168, 361, 74],\n",
              " [32, 1455],\n",
              " [32, 1455, 3],\n",
              " [32, 1455, 3, 26],\n",
              " [32, 1455, 3, 26, 102],\n",
              " [32, 1455, 3, 26, 102, 1456],\n",
              " [32, 1455, 3, 26, 102, 1456, 147],\n",
              " [907, 11],\n",
              " [907, 11, 187],\n",
              " [907, 11, 187, 26],\n",
              " [907, 11, 187, 26, 908],\n",
              " [907, 11, 187, 26, 908, 909],\n",
              " [1, 355],\n",
              " [1, 355, 1457],\n",
              " [1, 355, 1457, 2],\n",
              " [1, 355, 1457, 2, 910],\n",
              " [1, 355, 1457, 2, 910, 146],\n",
              " [1, 355, 1457, 2, 910, 146, 659],\n",
              " [1, 355, 1457, 2, 910, 146, 659, 1458],\n",
              " [911, 265],\n",
              " [911, 265, 169],\n",
              " [911, 265, 169, 8],\n",
              " [911, 265, 169, 8, 26],\n",
              " [911, 265, 169, 8, 26, 1459],\n",
              " [911, 265, 169, 8, 26, 1459, 167],\n",
              " [56, 430],\n",
              " [56, 430, 187],\n",
              " [56, 430, 187, 1460],\n",
              " [56, 430, 187, 1460, 26],\n",
              " [56, 430, 187, 1460, 26, 52],\n",
              " [56, 430, 187, 1460, 26, 52, 70],\n",
              " [912, 35],\n",
              " [912, 35, 26],\n",
              " [912, 35, 26, 422],\n",
              " [912, 35, 26, 422, 913],\n",
              " [18, 27],\n",
              " [18, 27, 34],\n",
              " [18, 27, 34, 1461],\n",
              " [18, 27, 34, 1461, 914],\n",
              " [18, 27, 34, 1461, 914, 11],\n",
              " [18, 27, 34, 1461, 914, 11, 527],\n",
              " [18, 27, 34, 1461, 914, 11, 527, 1462],\n",
              " [75, 1463],\n",
              " [75, 1463, 167],\n",
              " [75, 1463, 167, 65],\n",
              " [75, 1463, 167, 65, 1464],\n",
              " [75, 1463, 167, 65, 1464, 34],\n",
              " [75, 1463, 167, 65, 1464, 34, 2],\n",
              " [75, 1463, 167, 65, 1464, 34, 2, 97],\n",
              " [2, 51],\n",
              " [2, 51, 1465],\n",
              " [2, 51, 1465, 1466],\n",
              " [2, 51, 1465, 1466, 60],\n",
              " [2, 51, 1465, 1466, 60, 915],\n",
              " [2, 51, 1465, 1466, 60, 915, 41],\n",
              " [34, 26],\n",
              " [34, 26, 1467],\n",
              " [34, 26, 1467, 1468],\n",
              " [34, 26, 1467, 1468, 1],\n",
              " [34, 26, 1467, 1468, 1, 114],\n",
              " [34, 26, 1467, 1468, 1, 114, 258],\n",
              " [34, 26, 1467, 1468, 1, 114, 258, 362],\n",
              " [20, 10],\n",
              " [20, 10, 118],\n",
              " [20, 10, 118, 1469],\n",
              " [20, 10, 118, 1469, 8],\n",
              " [20, 10, 118, 1469, 8, 9],\n",
              " [20, 10, 118, 1469, 8, 9, 1470],\n",
              " [916, 35],\n",
              " [916, 35, 1471],\n",
              " [916, 35, 1471, 363],\n",
              " [916, 35, 1471, 363, 10],\n",
              " [916, 35, 1471, 363, 10, 1472],\n",
              " [916, 35, 1471, 363, 10, 1472, 16],\n",
              " [916, 35, 1471, 363, 10, 1472, 16, 660],\n",
              " [364, 3],\n",
              " [364, 3, 365],\n",
              " [364, 3, 365, 107],\n",
              " [364, 3, 365, 107, 1473],\n",
              " [364, 3, 365, 107, 1473, 10],\n",
              " [364, 3, 365, 107, 1473, 10, 364],\n",
              " [364, 3, 365, 107, 1473, 10, 364, 1474],\n",
              " [366, 11],\n",
              " [366, 11, 366],\n",
              " [366, 11, 366, 367],\n",
              " [366, 11, 366, 367, 14],\n",
              " [366, 11, 366, 367, 14, 289],\n",
              " [366, 11, 366, 367, 14, 289, 917],\n",
              " [366, 11, 366, 367, 14, 289, 917, 8],\n",
              " [366, 11, 366, 367, 14, 289, 917, 8, 289],\n",
              " [107, 368],\n",
              " [107, 368, 10],\n",
              " [107, 368, 10, 7],\n",
              " [107, 368, 10, 7, 25],\n",
              " [107, 368, 10, 7, 25, 10],\n",
              " [107, 368, 10, 7, 25, 10, 918],\n",
              " [107, 368, 10, 7, 25, 10, 918, 14],\n",
              " [107, 368, 10, 7, 25, 10, 918, 14, 1475],\n",
              " [36, 418],\n",
              " [36, 418, 918],\n",
              " [36, 418, 918, 11],\n",
              " [36, 418, 918, 11, 220],\n",
              " [36, 418, 918, 11, 220, 62],\n",
              " [36, 418, 918, 11, 220, 62, 1476],\n",
              " [42, 2],\n",
              " [42, 2, 73],\n",
              " [42, 2, 73, 919],\n",
              " [42, 2, 73, 919, 5],\n",
              " [42, 2, 73, 919, 5, 105],\n",
              " [42, 2, 73, 919, 5, 105, 1477],\n",
              " [42, 2, 73, 919, 5, 105, 1477, 920],\n",
              " [30, 1478],\n",
              " [30, 1478, 921],\n",
              " [30, 1478, 921, 33],\n",
              " [30, 1478, 921, 33, 1479],\n",
              " [30, 1478, 921, 33, 1479, 62],\n",
              " [30, 1478, 921, 33, 1479, 62, 661],\n",
              " [50, 33],\n",
              " [50, 33, 18],\n",
              " [50, 33, 18, 922],\n",
              " [50, 33, 18, 922, 431],\n",
              " [50, 33, 18, 922, 431, 19],\n",
              " [50, 33, 18, 922, 431, 19, 88],\n",
              " [50, 33, 18, 922, 431, 19, 88, 651],\n",
              " [8, 1480],\n",
              " [8, 1480, 2],\n",
              " [8, 1480, 2, 323],\n",
              " [8, 1480, 2, 323, 7],\n",
              " [8, 1480, 2, 323, 7, 10],\n",
              " [8, 1480, 2, 323, 7, 10, 358],\n",
              " [8, 1480, 2, 323, 7, 10, 358, 200],\n",
              " [528, 71],\n",
              " [528, 71, 66],\n",
              " [528, 71, 66, 1481],\n",
              " [528, 71, 66, 1481, 47],\n",
              " [528, 71, 66, 1481, 47, 662],\n",
              " [528, 71, 66, 1481, 47, 662, 3],\n",
              " [528, 71, 66, 1481, 47, 662, 3, 258],\n",
              " [1482, 168],\n",
              " [1482, 168, 8],\n",
              " [1482, 168, 8, 168],\n",
              " [1482, 168, 8, 168, 30],\n",
              " [1482, 168, 8, 168, 30, 923],\n",
              " [1482, 168, 8, 168, 30, 923, 1483],\n",
              " [911, 1484],\n",
              " [911, 1484, 1],\n",
              " [911, 1484, 1, 282],\n",
              " [911, 1484, 1, 282, 1],\n",
              " [911, 1484, 1, 282, 1, 221],\n",
              " [911, 1484, 1, 282, 1, 221, 884],\n",
              " [88, 23],\n",
              " [88, 23, 8],\n",
              " [88, 23, 8, 66],\n",
              " [88, 23, 8, 66, 66],\n",
              " [88, 23, 8, 66, 66, 924],\n",
              " [88, 23, 8, 66, 66, 924, 925],\n",
              " [88, 23, 8, 66, 66, 924, 925, 33],\n",
              " [88, 23, 8, 66, 66, 924, 925, 33, 324],\n",
              " [128, 926],\n",
              " [128, 926, 529],\n",
              " [128, 926, 529, 81],\n",
              " [128, 926, 529, 81, 188],\n",
              " [128, 926, 529, 81, 188, 530],\n",
              " [128, 926, 529, 81, 188, 530, 66],\n",
              " [927, 29],\n",
              " [927, 29, 3],\n",
              " [927, 29, 3, 19],\n",
              " [927, 29, 3, 19, 1485],\n",
              " [927, 29, 3, 19, 1485, 517],\n",
              " [927, 29, 3, 19, 1485, 517, 181],\n",
              " [927, 29, 3, 19, 1485, 517, 181, 204],\n",
              " [927, 29, 3, 19, 1485, 517, 181, 204, 189],\n",
              " [927, 29, 3, 19, 1485, 517, 181, 204, 189, 155],\n",
              " [13, 28],\n",
              " [13, 28, 12],\n",
              " [13, 28, 12, 290],\n",
              " [13, 28, 12, 290, 3],\n",
              " [13, 28, 12, 290, 3, 1486],\n",
              " [13, 28, 12, 290, 3, 1486, 16],\n",
              " [13, 28, 12, 290, 3, 1486, 16, 1487],\n",
              " [13, 28, 12, 290, 3, 1486, 16, 1487, 74],\n",
              " [7, 10],\n",
              " [7, 10, 1488],\n",
              " [7, 10, 1488, 118],\n",
              " [7, 10, 1488, 118, 8],\n",
              " [7, 10, 1488, 118, 8, 517],\n",
              " [7, 10, 1488, 118, 8, 517, 110],\n",
              " [325, 42],\n",
              " [325, 42, 10],\n",
              " [325, 42, 10, 1489],\n",
              " [325, 42, 10, 1489, 218],\n",
              " [325, 42, 10, 1489, 218, 1490],\n",
              " [325, 42, 10, 1489, 218, 1490, 3],\n",
              " [325, 42, 10, 1489, 218, 1490, 3, 199],\n",
              " [2, 94],\n",
              " [2, 94, 49],\n",
              " [2, 94, 49, 928],\n",
              " [2, 94, 49, 928, 19],\n",
              " [2, 94, 49, 928, 19, 75],\n",
              " [2, 94, 49, 928, 19, 75, 16],\n",
              " [2, 94, 49, 928, 19, 75, 16, 1491],\n",
              " [2, 94, 49, 928, 19, 75, 16, 1491, 1492],\n",
              " [2, 94],\n",
              " [2, 94, 49],\n",
              " [2, 94, 49, 21],\n",
              " [2, 94, 49, 21, 9],\n",
              " [2, 94, 49, 21, 9, 929],\n",
              " [2, 94, 49, 21, 9, 929, 1],\n",
              " [2, 94, 49, 21, 9, 929, 1, 70],\n",
              " [2, 94, 49, 21, 9, 929, 1, 70, 663],\n",
              " [7, 10],\n",
              " [7, 10, 37],\n",
              " [7, 10, 37, 217],\n",
              " [7, 10, 37, 217, 5],\n",
              " [7, 10, 37, 217, 5, 19],\n",
              " [7, 10, 37, 217, 5, 19, 148],\n",
              " [7, 10, 37, 217, 5, 19, 148, 428],\n",
              " [7, 10, 37, 217, 5, 19, 148, 428, 531],\n",
              " [27, 91],\n",
              " [27, 91, 1493],\n",
              " [27, 91, 1493, 929],\n",
              " [27, 91, 1493, 929, 105],\n",
              " [27, 91, 1493, 929, 105, 93],\n",
              " [27, 91, 1493, 929, 105, 93, 266],\n",
              " [30, 1494],\n",
              " [30, 1494, 51],\n",
              " [30, 1494, 51, 55],\n",
              " [30, 1494, 51, 55, 1495],\n",
              " [30, 1494, 51, 55, 1495, 432],\n",
              " [30, 1494, 51, 55, 1495, 432, 8],\n",
              " [30, 1494, 51, 55, 1495, 432, 8, 157],\n",
              " [114, 40],\n",
              " [114, 40, 144],\n",
              " [114, 40, 144, 1496],\n",
              " [114, 40, 144, 1496, 8],\n",
              " [114, 40, 144, 1496, 8, 2],\n",
              " [114, 40, 144, 1496, 8, 2, 94],\n",
              " [114, 40, 144, 1496, 8, 2, 94, 32],\n",
              " [114, 40, 144, 1496, 8, 2, 94, 32, 423],\n",
              " [1497, 18],\n",
              " [1497, 18, 26],\n",
              " [1497, 18, 26, 237],\n",
              " [1497, 18, 26, 237, 12],\n",
              " [1497, 18, 26, 237, 12, 70],\n",
              " [1497, 18, 26, 237, 12, 70, 2],\n",
              " [1497, 18, 26, 237, 12, 70, 2, 94],\n",
              " [1497, 18, 26, 237, 12, 70, 2, 94, 1498],\n",
              " [1497, 18, 26, 237, 12, 70, 2, 94, 1498, 28],\n",
              " [18, 133],\n",
              " [18, 133, 314],\n",
              " [18, 133, 314, 67],\n",
              " [18, 133, 314, 67, 8],\n",
              " [18, 133, 314, 67, 8, 2],\n",
              " [18, 133, 314, 67, 8, 2, 94],\n",
              " [18, 133, 314, 67, 8, 2, 94, 144],\n",
              " [18, 133, 314, 67, 8, 2, 94, 144, 222],\n",
              " [1, 664],\n",
              " [1, 664, 522],\n",
              " [1, 664, 522, 2],\n",
              " [1, 664, 522, 2, 1499],\n",
              " [1, 664, 522, 2, 1499, 20],\n",
              " [1, 664, 522, 2, 1499, 20, 1500],\n",
              " [1, 664, 522, 2, 1499, 20, 1500, 28],\n",
              " [37, 15],\n",
              " [37, 15, 1501],\n",
              " [37, 15, 1501, 223],\n",
              " [37, 15, 1501, 223, 8],\n",
              " [37, 15, 1501, 223, 8, 7],\n",
              " [37, 15, 1501, 223, 8, 7, 930],\n",
              " [37, 15, 1501, 223, 8, 7, 930, 1502],\n",
              " [7, 35],\n",
              " [7, 35, 931],\n",
              " [7, 35, 931, 83],\n",
              " [7, 35, 931, 83, 1503],\n",
              " [7, 35, 931, 83, 1503, 232],\n",
              " [7, 35, 931, 83, 1503, 232, 932],\n",
              " [12, 232],\n",
              " [12, 232, 933],\n",
              " [12, 232, 933, 7],\n",
              " [12, 232, 933, 7, 10],\n",
              " [12, 232, 933, 7, 10, 1504],\n",
              " [12, 232, 933, 7, 10, 1504, 15],\n",
              " [12, 232, 933, 7, 10, 1504, 15, 3],\n",
              " [12, 232, 933, 7, 10, 1504, 15, 3, 239],\n",
              " [88, 12],\n",
              " [88, 12, 118],\n",
              " [88, 12, 118, 54],\n",
              " [88, 12, 118, 54, 20],\n",
              " [88, 12, 118, 54, 20, 1505],\n",
              " [532, 42],\n",
              " [532, 42, 10],\n",
              " [532, 42, 10, 181],\n",
              " [532, 42, 10, 181, 10],\n",
              " [532, 42, 10, 181, 10, 54],\n",
              " [532, 42, 10, 181, 10, 54, 665],\n",
              " [532, 42, 10, 181, 10, 54, 665, 5],\n",
              " [532, 42, 10, 181, 10, 54, 665, 5, 188],\n",
              " [18, 7],\n",
              " [18, 7, 10],\n",
              " [18, 7, 10, 189],\n",
              " [18, 7, 10, 189, 368],\n",
              " [18, 7, 10, 189, 368, 13],\n",
              " [18, 7, 10, 189, 368, 13, 99],\n",
              " [18, 7, 10, 189, 368, 13, 99, 1506],\n",
              " [12, 10],\n",
              " [12, 10, 54],\n",
              " [12, 10, 54, 20],\n",
              " [12, 10, 54, 20, 934],\n",
              " [12, 10, 54, 20, 934, 11],\n",
              " [12, 10, 54, 20, 934, 11, 935],\n",
              " [12, 10, 54, 20, 934, 11, 935, 190],\n",
              " [7, 369],\n",
              " [7, 369, 118],\n",
              " [7, 369, 118, 10],\n",
              " [7, 369, 118, 10, 1507],\n",
              " [7, 369, 118, 10, 1507, 14],\n",
              " [7, 369, 118, 10, 1507, 14, 3],\n",
              " [7, 369, 118, 10, 1507, 14, 3, 1508],\n",
              " [1509, 7],\n",
              " [1509, 7, 260],\n",
              " [1509, 7, 260, 1510],\n",
              " [1509, 7, 260, 1510, 3],\n",
              " [1509, 7, 260, 1510, 3, 1511],\n",
              " [25, 3],\n",
              " [25, 3, 642],\n",
              " [25, 3, 642, 61],\n",
              " [25, 3, 642, 61, 21],\n",
              " [25, 3, 642, 61, 21, 9],\n",
              " [25, 3, 642, 61, 21, 9, 936],\n",
              " [25, 3, 642, 61, 21, 9, 936, 214],\n",
              " [58, 205],\n",
              " [58, 205, 9],\n",
              " [58, 205, 9, 138],\n",
              " [58, 205, 9, 138, 7],\n",
              " [58, 205, 9, 138, 7, 6],\n",
              " [58, 205, 9, 138, 7, 6, 93],\n",
              " [58, 205, 9, 138, 7, 6, 93, 205],\n",
              " [58, 205, 9, 138, 7, 6, 93, 205, 4],\n",
              " [58, 205, 9, 138, 7, 6, 93, 205, 4, 157],\n",
              " [46, 190],\n",
              " [46, 190, 21],\n",
              " [46, 190, 21, 666],\n",
              " [46, 190, 21, 666, 1512],\n",
              " [46, 190, 21, 666, 1512, 59],\n",
              " [46, 190, 21, 666, 1512, 59, 180],\n",
              " [46, 190, 21, 666, 1512, 59, 180, 15],\n",
              " [21, 22],\n",
              " [21, 22, 9],\n",
              " [21, 22, 9, 937],\n",
              " [21, 22, 9, 937, 13],\n",
              " [21, 22, 9, 937, 13, 429],\n",
              " [21, 22, 9, 937, 13, 429, 1],\n",
              " [21, 22, 9, 937, 13, 429, 1, 206],\n",
              " [36, 3],\n",
              " [36, 3, 118],\n",
              " [36, 3, 118, 106],\n",
              " [36, 3, 118, 106, 433],\n",
              " [36, 3, 118, 106, 433, 206],\n",
              " [36, 3, 118, 106, 433, 206, 1513],\n",
              " [36, 3, 118, 106, 433, 206, 1513, 204],\n",
              " [64, 19],\n",
              " [64, 19, 258],\n",
              " [64, 19, 258, 85],\n",
              " [64, 19, 258, 85, 12],\n",
              " [64, 19, 258, 85, 12, 15],\n",
              " [64, 19, 258, 85, 12, 15, 5],\n",
              " [64, 19, 258, 85, 12, 15, 5, 17],\n",
              " [7, 52],\n",
              " [7, 52, 70],\n",
              " [7, 52, 70, 93],\n",
              " [7, 52, 70, 93, 89],\n",
              " [7, 52, 70, 93, 89, 8],\n",
              " [7, 52, 70, 93, 89, 8, 62],\n",
              " [7, 52, 70, 93, 89, 8, 62, 36],\n",
              " [7, 52, 70, 93, 89, 8, 62, 36, 19],\n",
              " [22, 434],\n",
              " [22, 434, 22],\n",
              " [22, 434, 22, 10],\n",
              " [22, 434, 22, 10, 218],\n",
              " [22, 434, 22, 10, 218, 1514],\n",
              " [22, 434, 22, 10, 218, 1514, 20],\n",
              " [22, 434, 22, 10, 218, 1514, 20, 434],\n",
              " [22, 434, 22, 10, 218, 1514, 20, 434, 10],\n",
              " [22, 434, 22, 10, 218, 1514, 20, 434, 10, 667],\n",
              " [8, 66],\n",
              " [8, 66, 5],\n",
              " [8, 66, 5, 62],\n",
              " [8, 66, 5, 62, 34],\n",
              " [8, 66, 5, 62, 34, 7],\n",
              " [8, 66, 5, 62, 34, 7, 25],\n",
              " [8, 66, 5, 62, 34, 7, 25, 10],\n",
              " [8, 66, 5, 62, 34, 7, 25, 10, 1515],\n",
              " [1, 7],\n",
              " [1, 7, 313],\n",
              " [1, 7, 313, 283],\n",
              " [1, 7, 313, 283, 25],\n",
              " [1, 7, 313, 283, 25, 1516],\n",
              " [1, 7, 313, 283, 25, 1516, 10],\n",
              " [1, 7, 313, 283, 25, 1516, 10, 1517],\n",
              " [10, 207],\n",
              " [10, 207, 240],\n",
              " [10, 207, 240, 62],\n",
              " [10, 207, 240, 62, 27],\n",
              " [10, 207, 240, 62, 27, 10],\n",
              " [10, 207, 240, 62, 27, 10, 34],\n",
              " [10, 207, 240, 62, 27, 10, 34, 169],\n",
              " [10, 207, 240, 62, 27, 10, 34, 169, 1518],\n",
              " [1519, 262],\n",
              " [1519, 262, 1520],\n",
              " [1519, 262, 1520, 52],\n",
              " [1519, 262, 1520, 52, 1],\n",
              " [1519, 262, 1520, 52, 1, 512],\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Pad sequences and create predictors/labels\n",
        "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "predictors, label = input_sequences[:,:-1], input_sequences[:,-1]\n",
        "label = to_categorical(label, num_classes=total_words)"
      ],
      "metadata": {
        "id": "zZsc8jEV5Lw_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1s7vCNqs5as7",
        "outputId": "ee4dd4fd-2827-409e-8c4c-be0c02c05b1e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0, ...,    0,    0, 1375],\n",
              "       [   0,    0,    0, ...,    0,    0,   30],\n",
              "       [   0,    0,    0, ...,    0,   30, 1377],\n",
              "       ...,\n",
              "       [   0,    0,    0, ..., 3233,  488,  488],\n",
              "       [   0,    0,    0, ...,  488,  488, 3234],\n",
              "       [   0,    0,    0, ...,  488, 3234,   14]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5OrmzJw5cJ9",
        "outputId": "5e33bd3f-dcd6-4ca8-bcc8-5fda940e2bbd"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nStep 3: Padded Input Sequences (Sample)\")\n",
        "print(input_sequences[:5])  # Print a sample of padded sequences\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_GVJs-75ezT",
        "outputId": "7e47e832-5c08-4e4a-da06-2b17ff6018c3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 3: Padded Input Sequences (Sample)\n",
            "[[   0    0    0    0    0    0    0    0    0 1375 1376]\n",
            " [   0    0    0    0    0    0    0    0    0   30 1377]\n",
            " [   0    0    0    0    0    0    0    0   30 1377 1378]\n",
            " [   0    0    0    0    0    0    0    0    0   34  414]\n",
            " [   0    0    0    0    0    0    0    0   34  414  869]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print predictors and labels\n",
        "print(\"\\nStep 3: Predictors (Input) and Labels (Output) Sample\")\n",
        "print(\"Predictors (Sample):\\n\", predictors[:5])\n",
        "print(\"Labels (Sample):\\n\", label[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jSKHnfR5icH",
        "outputId": "7c952d78-12a5-4352-b808-ae7115f3d438"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 3: Predictors (Input) and Labels (Output) Sample\n",
            "Predictors (Sample):\n",
            " [[   0    0    0    0    0    0    0    0    0 1375]\n",
            " [   0    0    0    0    0    0    0    0    0   30]\n",
            " [   0    0    0    0    0    0    0    0   30 1377]\n",
            " [   0    0    0    0    0    0    0    0    0   34]\n",
            " [   0    0    0    0    0    0    0    0   34  414]]\n",
            "Labels (Sample):\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Define the LSTM model with Dropout (for regularization)\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=total_words, output_dim=150),\n",
        "    LSTM(200, return_sequences=True),\n",
        "    Dropout(0.2),  # Dropout for regularization\n",
        "    LSTM(150),\n",
        "    Dense(total_words, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "OwhfbCtc5rbe"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "HycDN_nN50zx"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nStep 4: Model Summary\")\n",
        "model.build(input_shape=(None, max_sequence_len - 1))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "_y2GWFOg6C4m",
        "outputId": "20c51c3a-9c07-48bc-d62c-f2e03603cdf9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 4: Model Summary\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m150\u001b[0m)             │         \u001b[38;5;34m485,250\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m200\u001b[0m)             │         \u001b[38;5;34m280,800\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m200\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m)                 │         \u001b[38;5;34m210,600\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3235\u001b[0m)                │         \u001b[38;5;34m488,485\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">485,250</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">280,800</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">210,600</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3235</span>)                │         <span style=\"color: #00af00; text-decoration-color: #00af00\">488,485</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,465,135\u001b[0m (5.59 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,465,135</span> (5.59 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,465,135\u001b[0m (5.59 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,465,135</span> (5.59 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nStep 5: Training the Model\")\n",
        "history = model.fit(predictors, label, epochs=100, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUQ8UYzV6L3n",
        "outputId": "a0f96141-e087-45d6-c369-8d4258051a48"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 5: Training the Model\n",
            "Epoch 1/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.0219 - loss: 7.0924\n",
            "Epoch 2/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.0253 - loss: 6.4820\n",
            "Epoch 3/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - accuracy: 0.0322 - loss: 6.3826\n",
            "Epoch 4/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.0387 - loss: 6.2616\n",
            "Epoch 5/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.0384 - loss: 6.1414\n",
            "Epoch 6/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.0438 - loss: 5.9589\n",
            "Epoch 7/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.0484 - loss: 5.8027\n",
            "Epoch 8/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.0523 - loss: 5.6290\n",
            "Epoch 9/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.0605 - loss: 5.4849\n",
            "Epoch 10/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.0699 - loss: 5.3109\n",
            "Epoch 11/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.0814 - loss: 5.1240\n",
            "Epoch 12/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.0921 - loss: 4.9456\n",
            "Epoch 13/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.1004 - loss: 4.8060\n",
            "Epoch 14/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.1061 - loss: 4.6261\n",
            "Epoch 15/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.1172 - loss: 4.4919\n",
            "Epoch 16/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.1378 - loss: 4.2999\n",
            "Epoch 17/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.1648 - loss: 4.1544\n",
            "Epoch 18/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.1861 - loss: 3.9971\n",
            "Epoch 19/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.2127 - loss: 3.8421\n",
            "Epoch 20/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.2328 - loss: 3.7006\n",
            "Epoch 21/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.2536 - loss: 3.5907\n",
            "Epoch 22/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.2838 - loss: 3.4503\n",
            "Epoch 23/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.3064 - loss: 3.3179\n",
            "Epoch 24/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.3313 - loss: 3.2015\n",
            "Epoch 25/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.3387 - loss: 3.1237\n",
            "Epoch 26/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.3691 - loss: 2.9778\n",
            "Epoch 27/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.3815 - loss: 2.9085\n",
            "Epoch 28/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.4000 - loss: 2.8211\n",
            "Epoch 29/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.4158 - loss: 2.7171\n",
            "Epoch 30/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.4361 - loss: 2.6395\n",
            "Epoch 31/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.4475 - loss: 2.5751\n",
            "Epoch 32/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.4658 - loss: 2.4960\n",
            "Epoch 33/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.4724 - loss: 2.4457\n",
            "Epoch 34/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.4852 - loss: 2.3675\n",
            "Epoch 35/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.5006 - loss: 2.2930\n",
            "Epoch 36/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5147 - loss: 2.2308\n",
            "Epoch 37/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.5235 - loss: 2.1739\n",
            "Epoch 38/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.5420 - loss: 2.1145\n",
            "Epoch 39/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5496 - loss: 2.0639\n",
            "Epoch 40/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.5626 - loss: 2.0105\n",
            "Epoch 41/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.5732 - loss: 1.9670\n",
            "Epoch 42/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5880 - loss: 1.8891\n",
            "Epoch 43/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.5956 - loss: 1.8473\n",
            "Epoch 44/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.6043 - loss: 1.7974\n",
            "Epoch 45/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.6140 - loss: 1.7689\n",
            "Epoch 46/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6258 - loss: 1.7077\n",
            "Epoch 47/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.6335 - loss: 1.6924\n",
            "Epoch 48/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.6433 - loss: 1.6406\n",
            "Epoch 49/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.6529 - loss: 1.6005\n",
            "Epoch 50/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.6602 - loss: 1.5462\n",
            "Epoch 51/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.6763 - loss: 1.5029\n",
            "Epoch 52/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6787 - loss: 1.4806\n",
            "Epoch 53/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.6874 - loss: 1.4471\n",
            "Epoch 54/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.6918 - loss: 1.4258\n",
            "Epoch 55/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.7019 - loss: 1.3820\n",
            "Epoch 56/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.7064 - loss: 1.3547\n",
            "Epoch 57/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.7168 - loss: 1.3306\n",
            "Epoch 58/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.7214 - loss: 1.2850\n",
            "Epoch 59/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.7293 - loss: 1.2483\n",
            "Epoch 60/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.7356 - loss: 1.2303\n",
            "Epoch 61/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.7314 - loss: 1.2306\n",
            "Epoch 62/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.7441 - loss: 1.1797\n",
            "Epoch 63/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.7425 - loss: 1.1748\n",
            "Epoch 64/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.7534 - loss: 1.1330\n",
            "Epoch 65/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.7580 - loss: 1.1090\n",
            "Epoch 66/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7594 - loss: 1.0899\n",
            "Epoch 67/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.7643 - loss: 1.0783\n",
            "Epoch 68/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.7710 - loss: 1.0526\n",
            "Epoch 69/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.7759 - loss: 1.0402\n",
            "Epoch 70/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.7781 - loss: 1.0045\n",
            "Epoch 71/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.7780 - loss: 1.0140\n",
            "Epoch 72/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.7809 - loss: 0.9910\n",
            "Epoch 73/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.7886 - loss: 0.9730\n",
            "Epoch 74/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.7937 - loss: 0.9598\n",
            "Epoch 75/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7923 - loss: 0.9401\n",
            "Epoch 76/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.7968 - loss: 0.9284\n",
            "Epoch 77/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8062 - loss: 0.8787\n",
            "Epoch 78/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8039 - loss: 0.8943\n",
            "Epoch 79/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.8036 - loss: 0.8654\n",
            "Epoch 80/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8015 - loss: 0.8657\n",
            "Epoch 81/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8096 - loss: 0.8368\n",
            "Epoch 82/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.8119 - loss: 0.8414\n",
            "Epoch 83/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8104 - loss: 0.8407\n",
            "Epoch 84/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.8176 - loss: 0.8151\n",
            "Epoch 85/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8155 - loss: 0.8086\n",
            "Epoch 86/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8199 - loss: 0.7840\n",
            "Epoch 87/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.8203 - loss: 0.7798\n",
            "Epoch 88/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8225 - loss: 0.7727\n",
            "Epoch 89/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8202 - loss: 0.7553\n",
            "Epoch 90/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.8206 - loss: 0.7855\n",
            "Epoch 91/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8256 - loss: 0.7514\n",
            "Epoch 92/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.8307 - loss: 0.7270\n",
            "Epoch 93/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.8278 - loss: 0.7369\n",
            "Epoch 94/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8319 - loss: 0.7206\n",
            "Epoch 95/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.8325 - loss: 0.7103\n",
            "Epoch 96/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8277 - loss: 0.7223\n",
            "Epoch 97/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8302 - loss: 0.7143\n",
            "Epoch 98/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.8339 - loss: 0.6940\n",
            "Epoch 99/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8336 - loss: 0.7029\n",
            "Epoch 100/100\n",
            "\u001b[1m484/484\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8304 - loss: 0.6981\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(seed_text, next_words, max_sequence_len):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\n",
        "        # Print the seed text at each generation step\n",
        "        print(f\"Seed Text (Generating): {seed_text}\")\n",
        "\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='pre')\n",
        "        predicted = np.argmax(model.predict(token_list, verbose=0), axis=-1)\n",
        "        output_word = tokenizer.index_word.get(predicted[0], '')\n",
        "        seed_text += \" \" + output_word\n",
        "    return seed_text"
      ],
      "metadata": {
        "id": "wTswbBbM7FRD"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Generate some text\n",
        "seed_text = \"Making a famine where\"\n",
        "print(\"\\nStep 7: Generated Text\")\n",
        "generated_text = generate_text(seed_text, 2, max_sequence_len)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvPcYaR97ZeH",
        "outputId": "56e7eff9-9325-4654-ca07-4e7980c4d245"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 7: Generated Text\n",
            "Seed Text (Generating): Making a famine where\n",
            "Seed Text (Generating): Making a famine where abundance\n",
            "Making a famine where abundance lies\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training history\n",
        "def plot_history(history):\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.title('Model Loss Over Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.show()\n",
        "\n",
        "plot_history(history)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "i7wWxisT7aaC",
        "outputId": "e29f585d-0ebf-40ae-8ffb-3bd25381a374"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZSRDtlZ67eer"
      },
      "execution_count": 36,
      "outputs": []
    }
  ]
}